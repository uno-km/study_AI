{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgdeP0T29TVqJ8orTcmo4f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"82qZ47Hg4ZeP"},"outputs":[],"source":["# 1. OpenAI GPT-2 ì›ë³¸ ì†ŒìŠ¤ì½”ë“œ ë‹¤ìš´ë¡œë“œ\n","!git clone -q https://github.com/openai/gpt-2.git\n","%cd gpt-2\n","\n","# 2. í•„ìš”í•œ íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ (124M ëª¨ë¸ì˜ vocabê³¼ encoder)\n","!python -m pip install -q requests tqdm\n","!python download_model.py 124M   # ìë™ìœ¼ë¡œ encoder.json, vocab.bpe ë‹¤ìš´ë¡œë“œ"]},{"cell_type":"code","source":["from importlib.metadata import version\n","print(\"tiktoken:\", version(\"tiktoken\"))\n","\n","text = \"Hello, world. Is this-- a test?\""],"metadata":{"id":"kW3H8vAD4jIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","print(integers)\n","print(tik_tokenizer.decode(integers))\n","print(\"vocab size:\", tik_tokenizer.n_vocab)"],"metadata":{"id":"ELsEPOa74kju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. encoder ëª¨ë“ˆ ì„í¬íŠ¸\n","import sys\n","sys.path.append(\"./src\")\n","from encoder import get_encoder\n","\n","# 4. ì›ë³¸ í† í¬ë‚˜ì´ì € ìƒì„± ë° í…ŒìŠ¤íŠ¸\n","orig_tokenizer = get_encoder(model_name=\"124M\", models_dir=\"models\")\n","orig_ids = orig_tokenizer.encode(text)\n","print(\"OpenAI ì›ë³¸ í† í¬ë‚˜ì´ì € ê²°ê³¼:\", orig_ids)\n","print(\"ë””ì½”ë”© í™•ì¸:\", orig_tokenizer.decode(orig_ids))"],"metadata":{"id":"MHpgXLRA4mPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import transformers\n","from transformers import GPT2Tokenizer, GPT2TokenizerFast\n","\n","print(\"transformers:\", transformers.__version__)\n","\n","hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","print(hf_tokenizer(text)[\"input_ids\"])\n","\n","hf_tokenizer_fast = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","print(hf_tokenizer_fast(text)[\"input_ids\"])"],"metadata":{"id":"_zivZdYF4ns6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import regex as re\n","import requests\n","import json\n","\n","class FinalGPT2Tokenizer:\n","    \"\"\"\n","    Grok & User & Gemini Collaboration\n","    The 'Unbeatable' Pure Python GPT-2 Tokenizer\n","    Date: 2025-11-30\n","    \"\"\"\n","    def __init__(self):\n","        # 1. Load Vocab & Merges (HuggingFace)\n","        self.encoder = json.loads(requests.get(\"https://huggingface.co/gpt2/resolve/main/vocab.json\").text)\n","        self.decoder = {v: k for k, v in self.encoder.items()}\n","\n","        merges = requests.get(\"https://huggingface.co/gpt2/resolve/main/merges.txt\").text.split(\"\\n\")[1:-1]\n","        self.bpe_ranks = {tuple(m.split()): i for i, m in enumerate(merges)}\n","\n","        self.cache = {}\n","\n","        # 2. Byte Encoder/Decoder (Reversible mapping)\n","        self.byte_encoder = self._bytes_to_unicode()\n","        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n","\n","        # 3. The \"True\" Regex (Matched with tiktoken logic)\n","        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[^\\r\\n\\p{L}\\p{N}]?[\\p{L}\\p{N}]+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\")\n","\n","    def _bytes_to_unicode(self):\n","        bs = list(range(ord(\"!\"), ord(\"~\")+1)) + list(range(ord(\"Â¡\"), ord(\"Â¬\")+1)) + list(range(ord(\"Â®\"), ord(\"Ã¿\")+1))\n","        cs = bs[:]\n","        n = 0\n","        for b in range(256):\n","            if b not in bs:\n","                bs.append(b)\n","                cs.append(256 + n)\n","                n += 1\n","        return dict(zip(bs, [chr(c) for c in cs]))\n","\n","    def _bpe(self, token):\n","        if token in self.cache:\n","            return self.cache[token]\n","\n","        word = tuple(token)\n","        pairs = set(zip(word[:-1], word[1:]))\n","\n","        if not pairs:\n","            return token\n","\n","        while True:\n","            # Find the pair with the lowest rank\n","            bigram = min(pairs, key=lambda x: self.bpe_ranks.get(x, float(\"inf\")))\n","\n","            # If the pair is not in the vocab, we are done\n","            if bigram not in self.bpe_ranks:\n","                break\n","\n","            a, b = bigram\n","            new_word = []\n","            i = 0\n","            while i < len(word):\n","                # Find occurrence of 'a'\n","                try:\n","                    j = word.index(a, i)\n","                except ValueError:\n","                    new_word.extend(word[i:])\n","                    break\n","\n","                new_word.extend(word[i:j])\n","                i = j\n","\n","                # Check if it's the pair (a, b)\n","                if i < len(word) - 1 and word[i] == a and word[i+1] == b:\n","                    new_word.append(a + b)\n","                    i += 2\n","                else:\n","                    new_word.append(word[i])\n","                    i += 1\n","\n","            word = tuple(new_word)\n","            if len(word) == 1:\n","                break\n","\n","            # Re-generate pairs for the next iteration\n","            pairs = set(zip(word[:-1], word[1:]))\n","\n","        result = \" \".join(word)\n","        self.cache[token] = result\n","        return result\n","\n","    def encode(self, text):\n","        ids = []\n","        for m in self.pat.finditer(text):\n","            piece = m.group()\n","            # Bytes -> Unicode -> BPE -> Token IDs\n","            token = \"\".join(self.byte_encoder[b] for b in piece.encode(\"utf-8\"))\n","            bpe_words = self._bpe(token).split()\n","            ids.extend(self.encoder[t] for t in bpe_words)\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \"\".join(self.decoder[i] for i in ids)\n","        # Robust decoding with fallback\n","        return bytearray(self.byte_decoder.get(c, 0) for c in text if c in self.byte_decoder).decode(\"utf-8\", errors=\"replace\")\n","\n","# --- Final Verification ---\n","if __name__ == \"__main__\":\n","    t = FinalGPT2Tokenizer()\n","    input_str = \"Hello, world. Is this-- a test?\"\n","    encoded = t.encode(input_str)\n","    decoded = t.decode(encoded)\n","\n","    print(f\"Input:   {input_str}\")\n","    print(f\"Encoded: {encoded}\")\n","    print(f\"Decoded: {decoded}\")\n","\n","    assert input_str == decoded\n","    print(\"\\nâœ¨ Verification Passed: The Code God bows to this implementation.\")"],"metadata":{"id":"vEHw_9G54o_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ê¸´ í…ìŠ¤íŠ¸ ë¡œë“œ\n","raw_text = ''\n","import requests, bs4; raw_text = bs4.BeautifulSoup(requests.get(\"https://uno-kim.tistory.com/431\", headers={\"User-Agent\": \"Mozilla/5.0\"}).text, \"html.parser\").select_one(\".contents_style\").get_text(\"\\n\").strip()\n","print(f\"ğŸ“„ Tistory ê¸€ììˆ˜: {len(raw_text):,} ì\")\n","\n","#ê²°ê³¼ : ğŸ“„ Tistory ê¸€ììˆ˜: 20,479 ì"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IpTo27Et8Gxg","executionInfo":{"status":"ok","timestamp":1764425240338,"user_tz":-540,"elapsed":2466,"user":{"displayName":"eunho kim","userId":"13883822672007691837"}},"outputId":"1fa0ca8f-04f3-4bfa-f6b0-f1af391f4924"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“„ Tistory ê¸€ììˆ˜: 20,479 ì\n"]}]},{"cell_type":"code","source":["import time\n","# ==========================================\n","# [2] ë²¤ì¹˜ë§ˆí¬ ì¤€ë¹„ (Setup)\n","# ==========================================\n","print(\"âš¡ Preparing Benchmark Arena...\")\n","\n","# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (The Verdict í…ìŠ¤íŠ¸ ì‚¬ìš©)\n","text_data = raw_text\n","# ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í…ìŠ¤íŠ¸ 10ë°° ë»¥íŠ€ê¸°\n","bench_text = text_data * 10\n","print(f\"ğŸ“Š Test Data Loaded: {len(bench_text):,} characters (The Verdict x 10)\")\n","\n","# 2. ëª¨ë¸ ë¡œë”©\n","print(\"â³ Loading Tokenizers...\")\n","models = {}\n","\n","# (A) Tiktoken (Baseline)\n","models['Tiktoken (Rust)'] = tiktoken.get_encoding(\"gpt2\")\n","\n","# (B) Hugging Face Fast (Rust)\n","models['HF Fast (Rust)'] = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","\n","# (C) Hugging Face Slow (Python)\n","models['HF Slow (Python)'] = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# (D) FinalGPT2Tokenizer (Custom Python)\n","print(\"   -> Initializing FinalGPT2Tokenizer...\")\n","start_init = time.perf_counter()\n","models['Final Custom'] = FinalGPT2Tokenizer()\n","print(f\"   -> Custom Init Time: {time.perf_counter() - start_init:.4f} sec\")"],"metadata":{"id":"Ne1n8euR4qQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# [3] ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (Execution)\n","# ==========================================\n","results = []\n","baseline_ids = None\n","\n","print(\"\\nğŸš€ Starting Benchmark Race (Encoding)...\")\n","\n","for name, tokenizer in models.items():\n","    print(f\"   -> Running {name}...\")\n","\n","    # 1. ì‹œê°„ ì¸¡ì • (Encode)\n","    start_time = time.perf_counter()\n","\n","    if name == 'Tiktoken (Rust)':\n","        ids = tokenizer.encode(bench_text, allowed_special={\"<|endoftext|>\"})\n","        baseline_ids = ids # ê¸°ì¤€ì  ì„¤ì •\n","    elif name == 'Final Custom':\n","        ids = tokenizer.encode(bench_text)\n","    else: # HF\n","        ids = tokenizer(bench_text)['input_ids']\n","\n","    end_time = time.perf_counter()\n","    elapsed = end_time - start_time\n","\n","    # 2. ê²€ì¦ (Decode & Round-trip)\n","    # Decode ì‹œê°„ì€ ì¸¡ì •ì—ì„œ ì œì™¸ (Encode ì„±ëŠ¥ì´ í•µì‹¬ì´ë¯€ë¡œ)\n","    if name == 'Tiktoken (Rust)':\n","        decoded = tokenizer.decode(ids)\n","    elif name == 'Final Custom':\n","        decoded = tokenizer.decode(ids)\n","    else:\n","        decoded = tokenizer.decode(ids)\n","\n","    # 3. ë°ì´í„° ì§‘ê³„\n","    match_baseline = (ids == baseline_ids) if baseline_ids else True\n","    is_valid = (decoded == bench_text) # ì›ë³¸ ë³µêµ¬ ê°€ëŠ¥ ì—¬ë¶€\n","\n","    results.append({\n","        \"Model\": name,\n","        \"Time (sec)\": elapsed,\n","        \"Tokens\": len(ids),\n","        \"Speed (Chars/sec)\": len(bench_text) / elapsed,\n","        \"Match Tiktoken?\": \"âœ… Yes\" if match_baseline else \"âŒ No\",\n","        \"Round-trip Valid?\": \"âœ… Yes\" if is_valid else \"âŒ No\"\n","    })"],"metadata":{"id":"YxRn98uw4sJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","# ==========================================\n","# [4] ê²°ê³¼ ì¶œë ¥ (The Verdict)\n","# ==========================================\n","df = pd.DataFrame(results)\n","df = df.set_index(\"Model\")\n","df[\"Time Ratio (x Slower)\"] = df[\"Time (sec)\"] / df.loc[\"Tiktoken (Rust)\", \"Time (sec)\"]\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ğŸ† BENCHMARK RESULTS ğŸ†\")\n","print(\"=\"*80)\n","print(df[[\"Tokens\", \"Match Tiktoken?\", \"Round-trip Valid?\", \"Time (sec)\", \"Time Ratio (x Slower)\"]].to_markdown())\n","print(\"=\"*80)\n","\n","# ìµœì¢… í‰ê°€\n","custom_row = df.loc['Final Custom']\n","if custom_row['Match Tiktoken?'] == \"âœ… Yes\":\n","    print(\"\\nğŸ‰ [Conclusion] ì¶•í•˜í•œë‹¤! ë„ˆì˜ í† í¬ë‚˜ì´ì €ëŠ” Tiktokenê³¼ ì™„ë²½í•˜ê²Œ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚´ë±‰ëŠ”ë‹¤.\")\n","    print(\"   ë¹„ë¡ ìˆœìˆ˜ íŒŒì´ì¬ì´ë¼ ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ, ë¡œì§ì˜ 'ì •í™•ì„±'ì€ ì‹ ì˜ ê²½ì§€ì— ë„ë‹¬í–ˆë‹¤.\")\n","else:\n","    print(\"\\nâš ï¸ [Conclusion] ì•„ì§ Tiktokenê³¼ ê²°ê³¼ê°€ ë‹¤ë¥¸ ë¶€ë¶„ì´ ìˆë‹¤. ë””ë²„ê¹…ì´ ë” í•„ìš”í•˜ë‹¤.\")"],"metadata":{"id":"tkJ1ybnF4tiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJjDCgug4u78"},"execution_count":null,"outputs":[]}]}